# TODO des fonctionnalités (pas obligatoires pour le MVP)

  - Remplacer HotData/ColdData par quelque chose comme AuchanData/MerchData
  - Prendre le max d'info sur les pages catégories d'Auchan et les rendre prioritaire par rapport au MerchData
  - Updater la base de données uniquement avec AuchanData. L'idée serait de parser Auchan souvent et Merchandising rarement. 
  - Paralléliser. Utiliser un channel au lieu d'une liste produits de retour dans crawlList ?


Design Notes
============

J'ai juste testé que ça compile :p. 

## HTTP

Bon j'ai pas mal douté pour la lib http à utiliser. Curl par défaut de scalpel a
l'air relou. J'ai fait un test avec `http-conduit` qui a l'air performant mais
bon c'est peut être une connerie. J'ai passé tout mon temps à faire une monad
qui préserve les cookies à coup de `StateT` et `Lens`.

Je débute en lenses, on peut peut être éviter si tu le sens pas trop. En gros tu
définis des getters/setters pour tes structures de donnés et après tu as plein
d'opérateurs de ouf.

<https://wiki.haskell.org/LensBeginnersCheatsheet>
<https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/a-little-lens-starter-tutorial>

Ça a l'air mega vaste mais c'est un grand classique de l'Haskell je crois.
J'ai pas lu le second lien en entier mais ça a l'air bien expliqué.

À changer peut être pour :

- `Network.Browser` <https://hackage.haskell.org/package/HTTP-4000.3.4/docs/Network-Browser.html>
  ça fait un peu comme ma monade, sûrement en mieux. Après je sais pas trop ce
  que ça fait pour partager les sockets ou quoi. C'est ce qui a l'air bien avec
  le `Manager` de `http-conduit`. À voir en détail si c'est vraiment plus
  performant ou si `Browser` fait déjà comme on veut
- `Network.Wreq.Session` <https://hackage.haskell.org/package/wreq-0.4.1.0/docs/Network-Wreq-Session.html>
  je suis pas trop fan qu'on n'aie pas le contrôle sur la session, on est obligé
  de la modifier à chaque requêtre, on ne peut pas la cloner


## Scraping

On peut l'appeller avec ce qu'on veut en appelant `parseTags :: StringLike str => str -> [Tag str]`
de `tagsoup` puis `scrape :: (Ord str, StringLike str) => Scraper str a -> [Tag str] -> Maybe a`. 

Il faudra ensuite faire des scrapers pour les pages individuelles et autre chose
pour itérer sur toutes les pages produit, si possible en parallèle.
